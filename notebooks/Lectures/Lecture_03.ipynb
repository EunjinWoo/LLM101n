{
  "cells": [
    {
      "metadata": {
        "id": "284bed26807861ab"
      },
      "cell_type": "markdown",
      "source": [
        "# Lecture 3: Multi-Layer Perceptrons\n",
        "\n",
        "In this lecture, we will introduce Multi-Layer Perceptrons (MLP).\n",
        "\n",
        "We will reproduce the following paper [A Neural Probabilistic Language Model](https://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)"
      ],
      "id": "284bed26807861ab"
    },
    {
      "metadata": {
        "id": "1df5ff9bcc4a9e24"
      },
      "cell_type": "markdown",
      "source": [
        "## Curse of Dimensionality\n",
        "\n",
        "For a character-level language model, the input vector is a one-hot vector of size 27 (26 characters + 1 space).\n",
        "\n",
        "If the model is not a character-level language model, (e.g., word-level language model), the input vector size is the size of the vocabulary which is usually very large (e.g., 20,000).\n",
        "\n",
        "This leads to the curse of dimensionality.\n",
        "\n",
        "**Solution**: Use a lower-dimensional representation of the input vector.\n",
        "\n",
        "The hypothesis is that similar words will have similar representations (e.g., dog and cat). Let's find a way to embed words into a lower-dimensional space.\n",
        "\n",
        "**Example**\n",
        "- The cat is walking in the bedroom. (Train data)\n",
        "- A dog was running in a room. (Train data)\n",
        "- The cat was running in a room. (Train data)\n",
        "- A dog is walking in a bedroom. (Train data)\n",
        "- A cat was running in a <?> (Test data)\n",
        "\n",
        "The model should be able to predict the word \"room\"(or the similar words) in the test data."
      ],
      "id": "1df5ff9bcc4a9e24"
    },
    {
      "metadata": {
        "id": "3091db96691073c9"
      },
      "cell_type": "markdown",
      "source": [
        "## MLP\n",
        "\n",
        "In the previous lecture, we have successfully implemented Bigram language model.\n",
        "In this lecture, we will implement a Multi-Layer Perceptron (MLP) language model.\n",
        "\n",
        "For practical reasons, let's use a character-level language model.\n",
        "\n",
        "![MLP](https://github.com/EunjinWoo/LLM101n/blob/master/assets/mlp.png?raw=1)"
      ],
      "id": "3091db96691073c9"
    },
    {
      "metadata": {
        "id": "8708acc13f08c1a2"
      },
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "id": "8708acc13f08c1a2"
    },
    {
      "metadata": {
        "id": "37a60786dfb334e7",
        "outputId": "e2b745aa-7b96-4392-9eb6-c126066c96c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from utils import load_text, set_seed\n",
        "%matplotlib inline"
      ],
      "id": "37a60786dfb334e7",
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8a23396add82>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "a5ecfeadcb656ced"
      },
      "cell_type": "markdown",
      "source": [
        "### Configuration"
      ],
      "id": "a5ecfeadcb656ced"
    },
    {
      "metadata": {
        "id": "initial_id"
      },
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class MLPConfig:\n",
        "    root_dir: str = os.getcwd() + \"/../../\"\n",
        "    dataset_path: str = \"data/names.txt\"\n",
        "\n",
        "    # Tokenizer\n",
        "    vocab_size: int = 0  # Set later\n",
        "\n",
        "    # Model\n",
        "    context_size: int = 3\n",
        "    d_embed: int = 2\n",
        "    d_hidden: int = 32\n",
        "\n",
        "    # Training\n",
        "    batch_size: int = 32\n",
        "    lr: float = 2e-4\n",
        "    max_steps: int = 10000\n",
        "\n",
        "    seed: int = 101\n",
        "\n",
        "config = MLPConfig()"
      ],
      "id": "initial_id",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "5e753e3f451b6b46"
      },
      "cell_type": "markdown",
      "source": [
        "### Reproducibility"
      ],
      "id": "5e753e3f451b6b46"
    },
    {
      "metadata": {
        "id": "95b8b6a2b1a7d33b"
      },
      "cell_type": "code",
      "source": [
        "set_seed(config.seed)\n",
        "generator = torch.Generator().manual_seed(config.seed)"
      ],
      "id": "95b8b6a2b1a7d33b",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "d949a0c1c1439c98"
      },
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "id": "d949a0c1c1439c98"
    },
    {
      "metadata": {
        "id": "631634a0a4e5f9dc"
      },
      "cell_type": "code",
      "source": [
        "# Load text and split by lines\n",
        "names = load_text(config.root_dir + config.dataset_path).splitlines()"
      ],
      "id": "631634a0a4e5f9dc",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "d4e7c1b4ba601a75"
      },
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "id": "d4e7c1b4ba601a75"
    },
    {
      "metadata": {
        "id": "dd74c6d04eef45f4"
      },
      "cell_type": "code",
      "source": [
        "chars = [chr(i) for i in range(97, 123)]  # all alphabet characters\n",
        "chars.insert(0, \".\")  # Add special token\n",
        "config.vocab_size = len(chars)\n",
        "str2idx = {char: idx for idx, char in enumerate(chars)}\n",
        "idx2str = {idx: char for char, idx in str2idx.items()}"
      ],
      "id": "dd74c6d04eef45f4",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3963012859d3b3ad"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        "\n",
        "We need to create a dataset of (Input, Target) pairs.\n",
        "- Input: current 3 characters\n",
        "- Output: next 1 character"
      ],
      "id": "3963012859d3b3ad"
    },
    {
      "metadata": {
        "id": "a1fa2464c368e34a"
      },
      "cell_type": "code",
      "source": [
        "def prepare_dataset(_names):\n",
        "    _inputs, _targets = [], []\n",
        "\n",
        "    for name in _names:\n",
        "        #print(name)\n",
        "        context = [0] * config.context_size  # How many characters do we take to predict the next character\n",
        "\n",
        "        for char in name + \".\":\n",
        "            idx = str2idx[char]\n",
        "            _inputs.append(context)\n",
        "            _targets.append(idx)\n",
        "            #print(''.join(idx2str[i] for i in context), '--->', idx2str[idx])\n",
        "            context = context[1:] + [idx]  # Shift the context by 1 character\n",
        "\n",
        "        #print(\"=\"*10)\n",
        "\n",
        "    _inputs = torch.tensor(_inputs)\n",
        "    _targets = torch.tensor(_targets)\n",
        "\n",
        "    return _inputs, _targets"
      ],
      "id": "a1fa2464c368e34a",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "469a5f98bb7fcbcd"
      },
      "cell_type": "code",
      "source": [
        "inputs, targets = prepare_dataset(names)\n",
        "\n",
        "print(f\"Number of Input, Target pairs: {len(inputs)}\")\n",
        "print(f\"Input shape: {inputs.shape}, Output shape: {targets.shape}\")\n",
        "print(f\"First (Input, Target): {inputs[0]}, {targets[0]}\")\n",
        "print(f\"Second (Input, Target): {inputs[1]}, {targets[1]}\")"
      ],
      "id": "469a5f98bb7fcbcd",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "7262c9fc27cdc08a"
      },
      "cell_type": "markdown",
      "source": [
        "### Model\n",
        "\n",
        "The model consists of the following components:\n",
        "- Embedding\n",
        "- Hidden Layer\n",
        "- Output Layer\n"
      ],
      "id": "7262c9fc27cdc08a"
    },
    {
      "metadata": {
        "id": "f5e39988bc299d7b"
      },
      "cell_type": "markdown",
      "source": [
        "#### Embedding\n",
        "\n",
        "Embedding is a lookup table that maps an input character to a lower-dimensional representation.\n",
        "\n",
        "Example\n",
        "- Input: 'a'\n",
        "- Output: [0.1, 0.2]\n",
        "\n",
        "[0.1, 0.2] is the represents the character 'a' in a lower-dimensional space."
      ],
      "id": "f5e39988bc299d7b"
    },
    {
      "metadata": {
        "id": "85c27cc98404dd9f"
      },
      "cell_type": "code",
      "source": [
        "# Embedding example\n",
        "C = torch.randn(27, 2)\n",
        "print(f\"Embedding shape: {C.shape}\")"
      ],
      "id": "85c27cc98404dd9f",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "17c51b98f3982eac"
      },
      "cell_type": "code",
      "source": [
        "# Embedding example\n",
        "# a: [1, :]\n",
        "a_embed = C[1, :]\n",
        "print(f\"Embedding of 'a': {a_embed}\")"
      ],
      "id": "17c51b98f3982eac",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "2d2b658e69d919a8"
      },
      "cell_type": "markdown",
      "source": [
        "How **Forward Pass** works in MLP:\n",
        "1. Embed the input characters.\n",
        "2. Concatenate the embeddings.\n",
        "3. Pass the concatenated embeddings through a hidden layer.\n",
        "4. Pass the hidden layer output through an output layer.\n",
        "5. Get the output of shape (vocab_size=27).\n",
        "\n",
        "\n",
        "Don't know how to concatenate? PyTorch provides concatenation functionality. [PyTorch Documentation](https://pytorch.org/docs/stable/generated/torch.cat.html)"
      ],
      "id": "2d2b658e69d919a8"
    },
    {
      "metadata": {
        "id": "6a9e6b3f7d5c8231"
      },
      "cell_type": "code",
      "source": [
        "# Example forward pass\n",
        "# Input: \".em\"\n",
        "e_embed = C[0]    # .: (embedding_size=2)\n",
        "m_embed = C[5]    # e: (embedding_size=2)\n",
        "m_embed2 = C[13]  # m: (embedding_size=2)\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Concatenate the embeddings                                                   #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "print(f\"Concatenated shape: {x.shape}\")"
      ],
      "id": "6a9e6b3f7d5c8231",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "f8177f096e7418b1"
      },
      "cell_type": "markdown",
      "source": [
        "#### Hidden Layer\n",
        "\n",
        "The hidden layer is a linear transformation followed by a non-linear activation function."
      ],
      "id": "f8177f096e7418b1"
    },
    {
      "metadata": {
        "id": "408d9c150ee5c42b"
      },
      "cell_type": "code",
      "source": [
        "# Hidden layer\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Implement the hidden layer                                                   #\n",
        "# Use tanh as your activation function.                                        #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "W1 = torch.randn(6,4)\n",
        "h = F.tanh(x @ W1)\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "print(f\"Hidden shape: {h.shape}\")"
      ],
      "id": "408d9c150ee5c42b",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "f841bf9f574977e9"
      },
      "cell_type": "markdown",
      "source": [
        "#### Output Layer\n",
        "\n",
        "To get the logits, we need to pass the hidden layer output through another linear transformation.\n",
        "\n",
        "The output layer is a linear transformation.\n",
        "\n",
        "Example\n",
        "- Input: any kind of vector\n",
        "- Output: a vector of size 27 (vocab_size) representing the probability of each character."
      ],
      "id": "f841bf9f574977e9"
    },
    {
      "metadata": {
        "id": "6ef79a57f1c08f5c"
      },
      "cell_type": "code",
      "source": [
        "# Output layer\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Implement the output layer                                                   #\n",
        "# Activation function must be ???                                              #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "W2 = torch.randn(4, 27)\n",
        "y = F.softmax(h @ W2)\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Output: {y}\")\n",
        "print(f\"Sum of probabilities: {y.sum()}\")"
      ],
      "id": "6ef79a57f1c08f5c",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "8ee0e192b1188559"
      },
      "cell_type": "code",
      "source": [
        "# Example of prediction\n",
        "print(f\"Input characters: {idx2str[0]}, {idx2str[5]}, {idx2str[13]}\")\n",
        "print(f\"Output character (probability): {idx2str[y.argmax().item()]}, {y.max()}\")"
      ],
      "id": "8ee0e192b1188559",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "7e3ed6aef1236744"
      },
      "cell_type": "markdown",
      "source": [
        "Let's refactor the code\n",
        "\n",
        "- Input:\n",
        "    - Shape: (batch_size, context_size)\n",
        "    - Example: [[5, 12, 12], [12, 12, 5]]  # \"emm\", \"mme\"\n",
        "\n",
        "- Parameters:\n",
        "    - Embedding:\n",
        "        - Shape: (vocab_size, d_embed)\n",
        "    - W1:\n",
        "        - Shape: (d_embed * context_size, d_hidden)\n",
        "    - W2:\n",
        "        - Shape: (d_hidden, vocab_size)\n",
        "\n",
        "- Output:\n",
        "    - Shape: (batch_size, vocab_size)\n",
        "    - Example: [[0.04, 0.03, ..., 0.02], [0.02, 0.03, ..., 0.04]]\n",
        "\n",
        "What is a **mini-batch**?\n",
        "- It is a subset of the dataset.\n",
        "- In practice, the dataset is too large to fit into memory. Therefore, we divide the dataset into mini-batches, then feed the model batch by batch.\n",
        "- batch_size: the number of samples in a mini-batch"
      ],
      "id": "7e3ed6aef1236744"
    },
    {
      "metadata": {
        "id": "fadace327323382"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Initialize the parameters                                                    #\n",
        "# C: (vocab_size, d_embed)                                                     #\n",
        "# W1: (d_embed * context_size, d_hidden)                                       #\n",
        "# b1: (d_hidden)                                                               #\n",
        "# W2: (d_hidden, vocab_size)                                                   #\n",
        "# b2: (vocab_size)                                                             #\n",
        "# Set requires_grad to True                                                    #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2]\n",
        "print(f\"Number of parameters: {sum(p.numel() for p in parameters)}\")"
      ],
      "id": "fadace327323382",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "bbc57ec6f727432b"
      },
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "PyTorch provides a CrossEntropyLoss function. [PyTorch Documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "\n",
        "**Note**: Softmax is already included in the CrossEntropyLoss function.\n",
        "\n",
        "Change the learning rate so that the loss graph looks as following\n",
        "\n",
        "![Loss](https://github.com/EunjinWoo/LLM101n/blob/master/assets/train_loss.png?raw=1)"
      ],
      "id": "bbc57ec6f727432b"
    },
    {
      "metadata": {
        "id": "e836179adeea5de1"
      },
      "cell_type": "code",
      "source": [
        "lr = config.lr  # Change learning rate\n",
        "steps = []\n",
        "losses = []\n",
        "\n",
        "for i in range(config.max_steps):\n",
        "    # Mini-batch\n",
        "    idx = torch.randint(0, len(inputs), (config.batch_size,))\n",
        "    batch_input = inputs[idx]  # (batch_size, context_size)\n",
        "    batch_target = targets[idx]  # (batch_size)\n",
        "    if i == 0:\n",
        "        print(f\"Input batch shape: {batch_input.shape}\")\n",
        "        print(f\"Target batch shape: {batch_target.shape}\")\n",
        "\n",
        "    # Forward pass\n",
        "    ################################################################################\n",
        "    # TODO:                                                                        #\n",
        "    # Implement the forward pass                                                   #\n",
        "    # 1. Embed the input characters.                                               #\n",
        "    # 2. Concatenate the embeddings.                                               #\n",
        "    # 3. Pass the concatenated embeddings through a hidden layer.                  #\n",
        "    # 4. Pass the hidden layer output through an output layer.                     #\n",
        "    # DO NOT INCLUDE SOFTMAX                                                       #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    # Embedding\n",
        "\n",
        "    # Concatenate\n",
        "\n",
        "    # Hidden layer\n",
        "\n",
        "    # Output layer\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    loss = F.cross_entropy(logits, batch_target)  # (batch_size)\n",
        "\n",
        "    # Backward pass\n",
        "    for param in parameters:\n",
        "        param.grad = None\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    for param in parameters:\n",
        "        param.data += -lr * param.grad\n",
        "\n",
        "    # Track loss\n",
        "    steps.append(i)\n",
        "    losses.append(loss.log10().item())\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(steps, losses)\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "id": "e836179adeea5de1",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "df72a0dca624d881"
      },
      "cell_type": "code",
      "source": [
        "# Visualization of the embedding matrix\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(C[:,0].data, C[:,1].data, s=200)  # dimensions of 0 and 1\n",
        "for i in range(C.shape[0]):\n",
        "    plt.text(C[i,0].item(), C[i,1].item(), idx2str[i], ha=\"center\", va=\"center\", color='white')\n",
        "plt.grid('minor')\n",
        "plt.show()"
      ],
      "id": "df72a0dca624d881",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "dfcd744c7b998412"
      },
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "id": "dfcd744c7b998412"
    },
    {
      "metadata": {
        "id": "abda449e10e08b5d"
      },
      "cell_type": "code",
      "source": [
        "def generate_name():\n",
        "    new_name = []\n",
        "    context = [0] * config.context_size\n",
        "\n",
        "    while True:\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # 1. Forward pass                                                              #\n",
        "        # 2. Sample the next token                                                     #\n",
        "        # 3. Decode the token                                                          #\n",
        "        # 4. Update the start_idx                                                      #\n",
        "        # 5. Break if the next character is \".\"                                        #\n",
        "        # Hint: Use F.softmax to get the probabilities                                 #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        # Forward pass\n",
        "\n",
        "        # Sample\n",
        "\n",
        "\n",
        "        # Update context\n",
        "\n",
        "        # Break if \".\"\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    return \"\".join(new_name)\n",
        "\n",
        "for _ in range(5):\n",
        "    print(generate_name())"
      ],
      "id": "abda449e10e08b5d",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "fee809d43627c44d"
      },
      "cell_type": "code",
      "source": [
        "# TODO: Change the learning rate to get better results"
      ],
      "id": "fee809d43627c44d",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}